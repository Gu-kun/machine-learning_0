{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## task05：模型融合\n\n## 1.学习目标\n- 对于多种调参完成的模型进行模型融合"},{"metadata":{},"cell_type":"markdown","source":"## 2.学习内容\n模型融合是比赛后期一个重要的环节，大体来说有如下的类型方式：\n\n1. 简单加权融合:\n  - 回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；\n  - 分类：投票（Voting)\n  - 综合：排序融合(Rank averaging)，log融合\n\n\n2. stacking/blending:\n  - 构建多层模型，并利用预测结果再拟合预测。\n\n\n3. boosting/bagging（在xgboost，Adaboost,GBDT中已经用到）:\n  - 多树的提升方法"},{"metadata":{},"cell_type":"markdown","source":"### 个人见解：模型融合是大后期的提分手段，一般需要求融合的模型原理和所喂的特征有所区别才能达到最好的效果\n- boosting/bagging的方法一般不会用在模型融合，其在树模型中已经运用了\n- 简单加权融合十分简单，易于理解，效果也不错\n  这种是最常见的融合方法，其可行的融合方法也有很多，比如根据结果的得分进行加权融合，还可以做Log，exp处理等。在做结果融合的时候，有一个很重要的条件是模型结果的得分要比较近似，然后结果的差异要比较大，这样的结果融合往往有比较好的效果提升。\n- stacking/blending的方法容易造成过拟合，但相对的效果也最好\n"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Stacking相关理论介绍\n- 简单来说 stacking 就是当用初始训练数据学习出若干个基学习器后，用这些基学习器预测训练集得到的预测结果，把这些预测结果作为新的训练集，来学习一个新的学习器。\n- 同理，要用这些基学习器预测测试集，得到预测结果，再喂入那个新的学习器，得到最终的预测结果。\n- 在stacking方法中，我们把基学习器叫做初级学习器，用于结合的学习器叫做次级学习器或元学习器（meta-learner），次级学习器用于训练的数据叫做次级训练集。\n- 次级训练集是在训练集上用初级学习器得到的。"},{"metadata":{},"cell_type":"markdown","source":"Stacking本质上就是这么直接的思路，但是直接这样有时对于如果训练集和测试集分布不那么一致的情况下是有一点问题的，其问题在于用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集，这样或许模型在测试集上的泛化能力或者说效果会有一定的下降，因此现在的问题变成了如何降低再训练的过拟合性。\n\n这里我们一般有两种方法：\n- 次级模型尽量选择简单的线性模型\n其在训练的时候，有m个初级模型，每个模型训练5次（如果五折的话），最后得到的输出结果还是一样的\n- 利用K折交叉验证"},{"metadata":{},"cell_type":"markdown","source":"## 3.代码示例"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 回归/分类概率-加权融合："},{"metadata":{"trusted":true},"cell_type":"code","source":"## 生成一些简单的样本数据，test_prei 代表第i个模型的预测值\ntest_pre1 = [1.2, 3.2, 2.1, 6.2]\ntest_pre2 = [0.9, 3.1, 2.0, 5.9]\ntest_pre3 = [1.1, 2.9, 2.2, 6.0]\n\n# y_test_true 代表数据的真实标签\ny_test_true = [1, 3, 2, 6] ","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n## 定义结果的加权平均函数\ndef Weighted_method(test_pre1,test_pre2,test_pre3,w=[1/3,1/3,1/3]):\n    Weighted_result = w[0]*pd.Series(test_pre1)+w[1]*pd.Series(test_pre2)+w[2]*pd.Series(test_pre3)\n    return Weighted_result","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n# 各模型的预测结果计算MAE\nprint('Pred1 MAE:',metrics.mean_absolute_error(y_test_true, test_pre1))\nprint('Pred2 MAE:',metrics.mean_absolute_error(y_test_true, test_pre2))\nprint('Pred3 MAE:',metrics.mean_absolute_error(y_test_true, test_pre3))","execution_count":3,"outputs":[{"output_type":"stream","text":"Pred1 MAE: 0.1750000000000001\nPred2 MAE: 0.07499999999999993\nPred3 MAE: 0.10000000000000009\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 根据加权计算MAE\nw = [0.3,0.4,0.3] # 定义比重权值\nWeighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)\nprint('Weighted_pre MAE:',metrics.mean_absolute_error(y_test_true, Weighted_pre))","execution_count":4,"outputs":[{"output_type":"stream","text":"Weighted_pre MAE: 0.05750000000000027\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"还有一些特殊的形式，比如mean平均，median平均"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 定义结果的加权平均函数\ndef Mean_method(test_pre1,test_pre2,test_pre3):\n    Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).mean(axis=1)\n    return Mean_result","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Mean_pre = Mean_method(test_pre1,test_pre2,test_pre3)\nprint('Mean_pre MAE:',metrics.mean_absolute_error(y_test_true, Mean_pre))","execution_count":6,"outputs":[{"output_type":"stream","text":"Mean_pre MAE: 0.06666666666666693\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 定义结果的加权平均函数\ndef Median_method(test_pre1,test_pre2,test_pre3):\n    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1)\n    return Median_result","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Median_pre = Median_method(test_pre1,test_pre2,test_pre3)\nprint('Median_pre MAE:',metrics.mean_absolute_error(y_test_true, Median_pre))","execution_count":8,"outputs":[{"output_type":"stream","text":"Median_pre MAE: 0.07500000000000007\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 回归/分类概率-Stacking融合："},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\ndef Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression()):\n    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=1).values,y_train_true)\n    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).values)\n    return Stacking_result","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 生成一些简单的样本数据，test_prei 代表第i个模型再训练集上的预测值\ntrain_reg1 = [3.2, 8.2, 9.1, 5.2]\ntrain_reg2 = [2.9, 8.1, 9.0, 4.9]\ntrain_reg3 = [3.1, 7.9, 9.2, 5.0]\n# y_test_true 代表训练集的标签\ny_train_true = [3, 8, 9, 5] \n\ntest_pre1 = [1.2, 3.2, 2.1, 6.2]\ntest_pre2 = [0.9, 3.1, 2.0, 5.9]\ntest_pre3 = [1.1, 2.9, 2.2, 6.0]\n\n# y_test_true 代表测试集的标签\ny_test_true = [1, 3, 2, 6] ","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_L2= linear_model.LinearRegression()\nStacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,\n                               test_pre1,test_pre2,test_pre3,model_L2)\nprint('Stacking_pre MAE:',metrics.mean_absolute_error(y_test_true, Stacking_pre))","execution_count":11,"outputs":[{"output_type":"stream","text":"Stacking_pre MAE: 0.042134831460675204\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"可以发现模型结果相对于之前有进一步的提升，这是我们需要注意的一点是，对于第二层Stacking的模型不宜选取的过于复杂，这样会导致模型在训练集上过拟合，从而使得在测试集上并不能达到很好的效果。"},{"metadata":{},"cell_type":"markdown","source":"## 3.3 分类模型加权融合—投票\n\nVoting即投票机制，分为软投票和硬投票两种，其原理采用少数服从多数的思想。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_blobs\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。\n\niris = datasets.load_iris()\n\nx=iris.data\ny=iris.target\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n\nclf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7,\n                     colsample_bytree=0.6, objective='binary:logistic')\nclf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n                              min_samples_leaf=63,oob_score=True)\nclf3 = SVC(C=0.1)\n\n# 硬投票\neclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='hard')\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","execution_count":13,"outputs":[{"output_type":"stream","text":"Accuracy: 0.95 (+/- 0.03) [XGBBoosting]\nAccuracy: 0.33 (+/- 0.00) [Random Forest]\nAccuracy: 0.92 (+/- 0.03) [SVM]\nAccuracy: 0.94 (+/- 0.02) [Ensemble]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 软投票：和硬投票原理相同，增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。\n\nx=iris.data\ny=iris.target\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n\nclf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.8,\n                     colsample_bytree=0.8, objective='binary:logistic')\nclf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n                              min_samples_leaf=63,oob_score=True)\nclf3 = SVC(C=0.1, probability=True)\n\n# 软投票\neclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='soft', weights=[2, 1, 1])\nclf1.fit(x_train, y_train)\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Ensemble']):\n    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","execution_count":14,"outputs":[{"output_type":"stream","text":"Accuracy: 0.96 (+/- 0.02) [XGBBoosting]\nAccuracy: 0.33 (+/- 0.00) [Random Forest]\nAccuracy: 0.92 (+/- 0.03) [SVM]\nAccuracy: 0.96 (+/- 0.02) [Ensemble]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 分类模型Stacking\\Blending融合"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5-Fold Stacking\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier\nimport pandas as pd\n#创建训练的数据集\ndata_0 = iris.data\ndata = data_0[:100,:]\n\ntarget_0 = iris.target\ntarget = target_0[:100]\n\n#模型融合中使用到的各个单模型\nclfs = [LogisticRegression(solver='lbfgs'),\n        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n \n#切分一部分数据作为测试集\nX, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n\ndataset_blend_train = np.zeros((X.shape[0], len(clfs)))\ndataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n\n#5折stacking\nn_splits = 5\nskf = StratifiedKFold(n_splits)\nskf = skf.split(X, y)\n\nfor j, clf in enumerate(clfs):\n    #依次训练各个单模型\n    dataset_blend_test_j = np.zeros((X_predict.shape[0], 5))\n    for i, (train, test) in enumerate(skf):\n        #5-Fold交叉训练，使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。\n        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n        clf.fit(X_train, y_train)\n        y_submission = clf.predict_proba(X_test)[:, 1]\n        dataset_blend_train[test, j] = y_submission\n        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n    #对于测试集，直接用这k个模型的预测值均值作为新的特征。\n    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n\nclf = LogisticRegression(solver='lbfgs')\nclf.fit(dataset_blend_train, y)\ny_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n\nprint(\"Val auc Score of Stacking: %f\" % (roc_auc_score(y_predict, y_submission)))\n","execution_count":15,"outputs":[{"output_type":"stream","text":"val auc Score: 1.000000\nval auc Score: 0.500000\nval auc Score: 0.500000\nval auc Score: 0.500000\nval auc Score: 0.500000\nVal auc Score of Stacking: 1.000000\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Blending，其实和Stacking是一种类似的多层模型融合的形式\n- 其主要思路是把原始的训练集先分成两部分，比如70%的数据作为新的训练集，剩下30%的数据作为测试集。\n- 在第一层，我们在这70%的数据上训练多个模型，然后去预测那30%数据的label，同时也预测test集的label。\n- 在第二层，我们就直接用这30%数据在第一层预测的结果做为新特征继续训练，然后用test集第一层预测的label做特征，用第二层训练的模型做进一步预测"},{"metadata":{},"cell_type":"markdown","source":"其优点在于：\n- 比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）\n- 避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集\n缺点在于：\n- 使用了很少的数据（第二阶段的blender只使用training set30%的量）\n- blender可能会过拟合\n- stacking使用多次的交叉验证会比较稳健 "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Blending\n\n \n#创建训练的数据集\ndata_0 = iris.data\ndata = data_0[:100,:]\n\ntarget_0 = iris.target\ntarget = target_0[:100]\n \n#模型融合中使用到的各个单模型\nclfs = [LogisticRegression(solver='lbfgs'),\n        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n        #ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n \n#切分一部分数据作为测试集\nX, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n\n#切分训练数据集为d1,d2两部分\nX_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020)\ndataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))\ndataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n \nfor j, clf in enumerate(clfs):\n    #依次训练各个单模型\n    clf.fit(X_d1, y_d1)\n    y_submission = clf.predict_proba(X_d2)[:, 1]\n    dataset_d1[:, j] = y_submission\n    #对于测试集，直接用这k个模型的预测值作为新的特征。\n    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]\n    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n\n#融合使用的模型\nclf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\nclf.fit(dataset_d1, y_d2)\ny_submission = clf.predict_proba(dataset_d2)[:, 1]\nprint(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission)))","execution_count":16,"outputs":[{"output_type":"stream","text":"val auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nval auc Score: 1.000000\nVal auc Score of Blending: 1.000000\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 4.总计\n融合是提分和提升模型鲁棒性的一种重要方法，再实践中主要有三种方法：\n- 结果层面的融合，这种是最常见的融合方法，其可行的融合方法也有很多，比如根据结果的得分进行加权融合，还可以做Log，exp处理等。在做结果融合的时候，有一个很重要的条件是模型结果的得分要比较近似，然后结果的差异要比较大，这样的结果融合往往有比较好的效果提升。\n- 特征层面的融合，这个层面其实感觉不叫融合，准确说可以叫分割，很多时候如果我们用同种模型训练，可以把特征进行切分给不同的模型，然后在后面进行模型或者结果融合有时也能产生比较好的效果。\n- 模型层面的融合，模型层面的融合可能就涉及模型的堆叠和设计，比如加Staking层，部分模型的结果作为特征输入等，这些就需要多实验和思考了，基于模型层面的融合最好不同模型类型要有一定的差异，用同种模型不同的参数的收益一般是比较小的。"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}